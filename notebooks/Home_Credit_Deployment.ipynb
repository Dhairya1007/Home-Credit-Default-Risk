{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRaoCJlB0v9Q"
      },
      "source": [
        "# Final Submission Case Study 1\n",
        "\n",
        "This notebook is my final submission for case study 1 from a deployment point of view. As discussed, this will focus on a new and more practical Deployment approach for this project. \n",
        "\n",
        "In this approach, I am only taking certain useful data from the user: specifically has current application data and a txt file upload for all his previous application data. \n",
        "\n",
        "Based on a pretrained model I will be making predictions on the likeliness that a user is a defaulter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxB48yTR1g-8"
      },
      "source": [
        "Hence, this notebook is divided into 2 parts:\n",
        "\n",
        "1. Model training for this approach. We will load the data, pre process the data, create new features based on Feature Engineering and finally train a model. \n",
        "\n",
        "For this notebook, I have chosen to go with testing the XGBoost and Lightgbm model as they performed best during my intial testing on the whole data.\n",
        "\n",
        "2. After the model training and file saving is comeplete, the next step would be to create a final submission in the given format. I will be creating 2 functions. \n",
        "\n",
        "Function 1 will take all the input data given as a single data point and output a prediction value.\n",
        "\n",
        "Function 2 will take in the input datapoint and the known target value and output a metric score, which is AUC Score in this case. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMXkFm-_2ew1"
      },
      "source": [
        "# Part 1 - Data Preperation, FE and Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P37bn2PmBhaA",
        "outputId": "e654743c-7542-4a48-fc16-2969f906740f"
      },
      "source": [
        "!pip install bayesian-optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp37-none-any.whl size=11687 sha256=66d813547ec34dfafe0385fe8bf2c93de8f578dc083591aacba41eb130a464e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5NZ9__Z1gT3"
      },
      "source": [
        "# importing the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgbm\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import os\n",
        "from time import time\n",
        "import itertools\n",
        "import gc\n",
        "from bayes_opt import BayesianOptimization\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Sklearn Imports\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfu13qzo9iWx",
        "outputId": "29c27ffe-90d2-433e-f9dd-c19865718f02"
      },
      "source": [
        "# Downloading and loading the data into disk\n",
        "!wget wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en-GB;q=0.9,en-US;q=0.8,en;q=0.7,hi;q=0.6\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/9120/860599/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1617701980&Signature=jZEuiUCD%2B%2FrSm3nJq5rSr10wK2nsJyIUcRxixH4pEy08y2O%2FlsSbEqrHxY44F8RJ%2Bf8tYeJ%2Bx8lw%2Fx%2Brk017uPrxOtczQq2iyqqJ0PlsHV%2FcHLuj060fGqthcsbfRIq14zZRMr1wrWEJXSWrBxMU20KRMeanWwLg4jgek3PoRR9u8uj720Mk4f9U50OlABb%2B3Wflk34zAexgD2QLviahtlNxQ9jpfxy8ao7edT7nD90Bft6U9UAoGX9lwOFOWX8SJCV%2BJDbXqTYQj%2FMMwrsvyqa7tHjo85g%2BK4%2BJUWzfA39irFiM063A3s0nghOPQD5qcjeEsxBb%2F9FJSI416S0yaA%3D%3D&response-content-disposition=attachment%3B+filename%3Dhome-credit-default-risk.zip\" -c -O 'home-credit-default-risk.zip'\n",
        "\n",
        "print('-'*100)\n",
        "print('Data Downloaded! Unzipping......')\n",
        "print('-'*100)\n",
        "print()\n",
        "\n",
        "# Once zip file is downloaded, just unzip and store the contents in the '/data' folder\n",
        "!unzip 'home-credit-default-risk.zip' -d '/data'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-05 09:02:14--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2021-04-05 09:02:15--  https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/9120/860599/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1617701980&Signature=jZEuiUCD%2B%2FrSm3nJq5rSr10wK2nsJyIUcRxixH4pEy08y2O%2FlsSbEqrHxY44F8RJ%2Bf8tYeJ%2Bx8lw%2Fx%2Brk017uPrxOtczQq2iyqqJ0PlsHV%2FcHLuj060fGqthcsbfRIq14zZRMr1wrWEJXSWrBxMU20KRMeanWwLg4jgek3PoRR9u8uj720Mk4f9U50OlABb%2B3Wflk34zAexgD2QLviahtlNxQ9jpfxy8ao7edT7nD90Bft6U9UAoGX9lwOFOWX8SJCV%2BJDbXqTYQj%2FMMwrsvyqa7tHjo85g%2BK4%2BJUWzfA39irFiM063A3s0nghOPQD5qcjeEsxBb%2F9FJSI416S0yaA%3D%3D&response-content-disposition=attachment%3B+filename%3Dhome-credit-default-risk.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.103.128, 142.250.128.128, 142.251.6.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.103.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 721616255 (688M) [application/zip]\n",
            "Saving to: ‘home-credit-default-risk.zip’\n",
            "\n",
            "home-credit-default 100%[===================>] 688.19M   127MB/s    in 6.4s    \n",
            "\n",
            "2021-04-05 09:02:21 (108 MB/s) - ‘home-credit-default-risk.zip’ saved [721616255/721616255]\n",
            "\n",
            "FINISHED --2021-04-05 09:02:21--\n",
            "Total wall clock time: 6.6s\n",
            "Downloaded: 1 files, 688M in 6.4s (108 MB/s)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Data Downloaded! Unzipping......\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Archive:  home-credit-default-risk.zip\n",
            "  inflating: /data/HomeCredit_columns_description.csv  \n",
            "  inflating: /data/POS_CASH_balance.csv  \n",
            "  inflating: /data/application_test.csv  \n",
            "  inflating: /data/application_train.csv  \n",
            "  inflating: /data/bureau.csv        \n",
            "  inflating: /data/bureau_balance.csv  \n",
            "  inflating: /data/credit_card_balance.csv  \n",
            "  inflating: /data/installments_payments.csv  \n",
            "  inflating: /data/previous_application.csv  \n",
            "  inflating: /data/sample_submission.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNdzG4yNZNUd"
      },
      "source": [
        "def one_hot_encoder(df, nan_category = True):\n",
        "    # List of all original Columns in the dataframe\n",
        "    original_columns = list(df.columns)\n",
        "    # List of all categorical features\n",
        "    categorical_columns = list(df.select_dtypes('object').columns)\n",
        "    # Doing one-hot encoding for categorical features\n",
        "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_category)\n",
        "    # Creating a list of new column names for better reference. \n",
        "    new_columns = [col for col in df.columns if col not in original_columns]\n",
        "    return df, new_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t32ha1nr0r8S"
      },
      "source": [
        "# This class will be used to train a LGBM and XGBOOST model for deployment\n",
        "class deployment_model_training:\n",
        "  '''\n",
        "  This class helps you train a model for deployment. \n",
        "  It contains the following functions:\n",
        "    1. init (initialization)\n",
        "    2. load_data\n",
        "    3. preprocessing\n",
        "    4. XGB_Model\n",
        "    5. LGB_Model\n",
        "    6. save_pickles\n",
        "  '''\n",
        "\n",
        "  # Function 1 : Initialization\n",
        "  def __init__(self, file_directory = ''):\n",
        "    '''\n",
        "        This function is used to initialize the Class members. \n",
        "        \n",
        "        Inputs:\n",
        "            self\n",
        "            file_directory: str, default = ''\n",
        "                Path of the directory where the file is stored\n",
        "        \n",
        "        Returns:\n",
        "            None\n",
        "    '''\n",
        "    self.file_directory = file_directory\n",
        "  \n",
        "  # Function 2 : Load Data\n",
        "  def load_data(self, verbose = False):\n",
        "    '''\n",
        "    This functions helps in loading all the required data for model training. \n",
        "    In this case, it will load the application train, test and previous application data.\n",
        "\n",
        "    Inputs: \n",
        "      self\n",
        "    \n",
        "    Returns:\n",
        "      None\n",
        "    '''\n",
        "    if verbose:\n",
        "      print('Please wait: Loading the data into memory...')\n",
        "      start_time = time()\n",
        "\n",
        "    # Loading the 3 datafiles into memory. \n",
        "    self.app_train = pd.read_csv(self.file_directory + 'application_train.csv')\n",
        "    self.app_test = pd.read_csv(self.file_directory + 'application_test.csv')\n",
        "    self.prev_app = pd.read_csv(self.file_directory + 'previous_application.csv')\n",
        "\n",
        "    # mandatory print statement\n",
        "    print('Data successfully loaded!')\n",
        "\n",
        "    if verbose:\n",
        "      print('It took {} seconds to load the data.'.format(round(time() - start_time, 2)))\n",
        "  \n",
        "  # Function 3 : Data Pre processing\n",
        "  def preprocessing(self, verbose = False):\n",
        "    '''\n",
        "    This function preprocesses the loaded data. The processing includes:\n",
        "      1. Choosing only model specific features (already chosen)\n",
        "      2. Handling Outliers and missing values \n",
        "      3. Categorical features encoding (one-hot)\n",
        "      4. Feature Engineering to create some better features\n",
        "\n",
        "    Inputs:\n",
        "      self\n",
        "      verbose : Whether to print statements (default : False)\n",
        "    \n",
        "    Returns:\n",
        "      train_df : The final processed data ready for model training\n",
        "      test_df : The final processed data ready for model testing (without target value)\n",
        "    '''\n",
        "\n",
        "    # Task 1 : Choose only the important features\n",
        "    # --------------------------------------------------------------------------------------------------------------\n",
        "    # i) Application Train, Test\n",
        "    chosen_features_app = ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_ID_PUBLISH', 'CODE_GENDER',\n",
        "                           'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'AMT_INCOME_TOTAL',\n",
        "                           'CNT_FAM_MEMBERS', 'FLAG_OWN_CAR', 'OWN_CAR_AGE', 'CNT_CHILDREN',\n",
        "                           'SK_ID_CURR', 'TARGET']\n",
        "    \n",
        "    # ii) Previous Application\n",
        "    chosen_features_prev = ['AMT_APPLICATION', 'AMT_CREDIT','AMT_DOWN_PAYMENT','AMT_GOODS_PRICE', 'AMT_ANNUITY',\n",
        "                            'HOUR_APPR_PROCESS_START', 'RATE_DOWN_PAYMENT','DAYS_DECISION', 'CNT_PAYMENT',\n",
        "                            'NAME_CONTRACT_STATUS', 'AMT_GOODS_PRICE', 'SK_ID_CURR', 'SK_ID_PREV']\n",
        "    \n",
        "    # Only get the chosen columns from the data\n",
        "\n",
        "    deployment_app_train = self.app_train[chosen_features_app]\n",
        "\n",
        "    # Removing the Target variable\n",
        "    chosen_features_app.remove('TARGET')\n",
        "\n",
        "    deployment_app_test = self.app_test[chosen_features_app]\n",
        "    deployment_prev_app = self.prev_app[chosen_features_prev]\n",
        "\n",
        "    # --------------------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    # Task 2 : Handling missing values and outliers\n",
        "    # --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    if verbose:\n",
        "        print('Handling Missing values and outliers......')\n",
        "    \n",
        "    # i) Application Train/Test Data\n",
        "\n",
        "    start = time()\n",
        "    # Merging train and test data for easier processing\n",
        "    merged_data = pd.concat([deployment_app_train, deployment_app_test], axis = 0, ignore_index = True)\n",
        "\n",
        "    # First, removing the XNA gender category. It only has 4 occurences. \n",
        "    merged_data.drop(merged_data[merged_data['CODE_GENDER'] == 'XNA'].index, inplace = True)\n",
        "    # 1. A specific value for days employed. (gives you around 1000 years, which is not possible)\n",
        "    merged_data['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n",
        "    # 4. AMT_INCOME Total greater than 10,000,000 (10 million) is highly unlikely (to default or in general). So, replace it\n",
        "    merged_data.loc[merged_data['AMT_INCOME_TOTAL'] > 1e7, 'AMT_INCOME_TOTAL'] = np.nan\n",
        "\n",
        "\n",
        "    # ii) Previous Application Data\n",
        "\n",
        "    deployment_prev_app.loc[deployment_prev_app['AMT_CREDIT'] > 6000000, 'AMT_CREDIT'] = np.nan\n",
        "\n",
        "    # --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    # Task 3 : Categorical One Hot encoding\n",
        "    # --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    # One hot encoding categorical features for application data\n",
        "    merged_data, merged_data_cat_columns = one_hot_encoder(merged_data)\n",
        "\n",
        "    # one hot encoding for categorical features, if any for previous application data\n",
        "    deployment_prev_app, prev_app_cat_features = one_hot_encoder(deployment_prev_app)\n",
        "\n",
        "    if verbose:\n",
        "      print('Done with pre processing! Time Elapsed : {} seconds'.format(round(time() - start, 2)))\n",
        "    # --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    # Task 4 : Feature Engineering\n",
        "    # --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    start = time()\n",
        "    if verbose: \n",
        "      print('Creating new and better features......')\n",
        "\n",
        "    # i) Application_train/test Feature Engineering\n",
        "    merged_data['NEW_CREDIT_TO_ANNUITY_RATIO'] = merged_data['AMT_CREDIT'] / merged_data['AMT_ANNUITY']\n",
        "    merged_data['NEW_CREDIT_TO_GOODS_RATIO'] = merged_data['AMT_CREDIT'] / merged_data['AMT_GOODS_PRICE']\n",
        "    merged_data['AGE'] =  pd.to_numeric((merged_data['DAYS_BIRTH'] / -365), downcast='integer')\n",
        "    merged_data['YEARS_EMPLOYED'] =  pd.to_numeric((merged_data['DAYS_EMPLOYED'] / -365), downcast='integer')\n",
        "    merged_data['INCOME_PER_PERSON'] = merged_data['AMT_INCOME_TOTAL'] / merged_data['CNT_FAM_MEMBERS']\n",
        "    merged_data['ANNUITY_INCOME_PERC'] = merged_data['AMT_ANNUITY'] / merged_data['AMT_INCOME_TOTAL']\n",
        "    merged_data['INCOME_CREDIT_PERC'] = merged_data['AMT_INCOME_TOTAL'] / merged_data['AMT_CREDIT']\n",
        "    merged_data['DAYS_EMPLOYED_PERC'] = merged_data['DAYS_EMPLOYED'] / merged_data['DAYS_BIRTH']\n",
        "    merged_data['PAYMENT_RATE'] = merged_data['AMT_ANNUITY'] / merged_data['AMT_CREDIT']\n",
        "\n",
        "    merged_data.drop(columns = ['DAYS_BIRTH', 'DAYS_EMPLOYED'], inplace = True)\n",
        "    # ii) Previous Application Data\n",
        "    deployment_prev_app['APP_CREDIT_PERC'] = deployment_prev_app['AMT_APPLICATION'] / deployment_prev_app['AMT_CREDIT']\n",
        "\n",
        "    # Aggregations we will perform \n",
        "    prev_app_num_agg = {\n",
        "                    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
        "                    'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
        "                    'AMT_CREDIT': ['min', 'max', 'mean'],\n",
        "                    'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
        "                    'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "                    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
        "                    'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "                    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
        "                    'CNT_PAYMENT': ['mean', 'sum']\n",
        "                   }\n",
        "\n",
        "    # Perform the aggregations based on the sk_id_curr\n",
        "    prev_agg_features = deployment_prev_app.groupby('SK_ID_CURR').agg(prev_app_num_agg)\n",
        "    prev_agg_features.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg_features.columns.tolist()])\n",
        "\n",
        "    # Previous Applications: Approved Applications - only numerical features\n",
        "    approved_applications = deployment_prev_app[deployment_prev_app['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
        "    approved_agg_features = approved_applications.groupby('SK_ID_CURR').agg(prev_app_num_agg)\n",
        "    approved_agg_features.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg_features.columns.tolist()])\n",
        "    prev_agg_features = prev_agg_features.join(approved_agg_features, how='left', on='SK_ID_CURR')\n",
        "    \n",
        "    # Previous Applications: Refused Applications - only numerical features\n",
        "    refused_applications = deployment_prev_app[deployment_prev_app['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
        "    refused_agg_features = refused_applications.groupby('SK_ID_CURR').agg(prev_app_num_agg)\n",
        "    refused_agg_features.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg_features.columns.tolist()])\n",
        "    prev_agg_features = prev_agg_features.join(refused_agg_features, how='left', on='SK_ID_CURR')\n",
        "\n",
        "    del refused_applications, refused_agg_features, approved_applications, approved_agg_features, deployment_prev_app\n",
        "    gc.collect()\n",
        "\n",
        "    if verbose:\n",
        "      print('Done with Feature Engineering! Time Elapsed : {} seconds'.format(round(time() - start, 2)))\n",
        "    \n",
        "    # Getting the Final train and test data ready\n",
        "    print(merged_data.shape)\n",
        "    print(prev_agg_features.shape)\n",
        "\n",
        "    # i) Merging both data files\n",
        "    final_df = merged_data.merge(prev_agg_features, how = 'left', on = 'SK_ID_CURR')\n",
        "\n",
        "    # ii) Dividing them into train and test data\n",
        "    train_df = final_df[final_df['TARGET'].notnull()]\n",
        "    test_df = final_df[final_df['TARGET'].isnull()]\n",
        "\n",
        "    # Remove the SK_ID_CURR Parameter\n",
        "    _ = train_df.pop('SK_ID_CURR')\n",
        "    _ = test_df.pop('SK_ID_CURR')\n",
        "    \n",
        "    if verbose:\n",
        "      print('The shape of Final training data : {}'.format(train_df.shape))\n",
        "      print('The shape of Final test data : {}'.format(test_df.shape))\n",
        "    \n",
        "    return train_df, test_df\n",
        "\n",
        "  # -------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  #---------------------------------------------------------------------------------------------------------------\n",
        "  # Function 4 : Light GBM Model training\n",
        "  def lgbm_model(self, train_df, test_df, nfolds = 3):\n",
        "    '''\n",
        "    This function trains a Light GBM Model.\n",
        "\n",
        "    Inputs:\n",
        "      self\n",
        "      train : Train data (includes TARGET)\n",
        "      test : Test Data\n",
        "      nfolds : Number of folds (default : 3)\n",
        "    \n",
        "    output: \n",
        "      trained model\n",
        "    '''\n",
        "\n",
        "    # Parameters for our LGBM Classifier\n",
        "    lgbm_params = {\n",
        "                'njobs': -1,\n",
        "                'n_estimators': 10000,\n",
        "                'learning_rate': .02,\n",
        "                'num_leaves': 34,\n",
        "                'colsample_bytree': .9497036,\n",
        "                'subsample': .8715623,\n",
        "                'max_depth': 7,\n",
        "                'reg_alpha': .041545473,\n",
        "                'reg_lambda': .0735294,\n",
        "                'min_split_gain': .0222415,\n",
        "                'min_child_weight': 39.3259775,\n",
        "                'silent': -1,\n",
        "                'verbose': -1\n",
        "    }\n",
        "    fold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
        "    a = 0\n",
        "\n",
        "    importances = pd.DataFrame()\n",
        "    y = train_df.pop('TARGET')\n",
        "\n",
        "    X = train_df\n",
        "    _ = test_df.pop('TARGET')\n",
        "    X_test = test_df\n",
        "\n",
        "    train_predict = np.zeros(train_df.shape[0])\n",
        "    cv_predict = np.zeros(train_df.shape[0])\n",
        "    test_predict = np.zeros(test_df.shape[0])\n",
        "\n",
        "    for i,(train, cv) in enumerate(fold.split(X,y)):\n",
        "      X_train, Y_train = X.iloc[train], y.iloc[train]\n",
        "      X_valid, Y_valid = X.iloc[cv], y.iloc[cv]\n",
        "\n",
        "      model = lgbm.LGBMClassifier(**lgbm_params)\n",
        "      \n",
        "      model.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
        "                eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n",
        "      \n",
        "      a=a+1\n",
        "\n",
        "      train_predict[train] = model.predict_proba(X_train, num_iteration=model.best_iteration_)[:, 1]\n",
        "      cv_predict[cv]=model.predict_proba(X_valid, num_iteration=model.best_iteration_)[:, 1]\n",
        "      \n",
        "      importances[\"imp\"] = model.feature_importances_\n",
        "      importances[\"fold\"] = i + 1\n",
        "      print('Fold ',i + 1,' TRAIN AUC : ',roc_auc_score(Y_train, train_predict[train]))\n",
        "      print('Fold ',i + 1,' CV AUC : ',roc_auc_score(Y_valid, cv_predict[cv]))\n",
        "            \n",
        "      del X_train, Y_train, X_valid, Y_valid\n",
        "      gc.collect()\n",
        "\n",
        "    # return a trained model\n",
        "    return model\n",
        "\n",
        "  def xgb_model(self, train, test, nfolds = 3):\n",
        "    '''\n",
        "    This function trains a Light GBM Model.\n",
        "\n",
        "    Inputs:\n",
        "      self\n",
        "      train : Train data (includes TARGET)\n",
        "      test : Test Data\n",
        "      nfolds : Number of folds (default : 3)\n",
        "    \n",
        "    output: \n",
        "      trained model\n",
        "    '''\n",
        "    fold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
        "    a = 0\n",
        "\n",
        "    importances = pd.DataFrame()\n",
        "    y = train_df.pop('TARGET')\n",
        "\n",
        "    X = train_df\n",
        "    _ = test_df.pop('TARGET')\n",
        "    X_test = test_df\n",
        "\n",
        "    train_predict = np.zeros(train_df.shape[0])\n",
        "    cv_predict = np.zeros(train_df.shape[0])\n",
        "    test_predict = np.zeros(test_df.shape[0])\n",
        "\n",
        "    for i,(train, cv) in enumerate(fold.split(X,y)):\n",
        "      X_train, Y_train = X.iloc[train], y.iloc[train]\n",
        "      X_valid, Y_valid = X.iloc[cv], y.iloc[cv]\n",
        "\n",
        "      model = xgb.XGBClassifier(learning_rate =0.01, n_estimators=10000, max_depth=4, min_child_weight=5, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', \n",
        "            nthread=4, scale_pos_weight=2.5, seed=27, reg_lambda = 1.2, verbose = 5, tree_method = 'gpu_hist')\n",
        "      \n",
        "      model.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
        "                eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n",
        "      \n",
        "      a=a+1\n",
        "\n",
        "      train_predict[train] = model.predict_proba(X_train)[:, 1]\n",
        "      cv_predict[cv]=model.predict_proba(X_valid)[:, 1]\n",
        "      \n",
        "      importances[\"imp\"] = model.feature_importances_\n",
        "      importances[\"fold\"] = i + 1\n",
        "      print('Fold ',i + 1,' TRAIN AUC : ',roc_auc_score(Y_train, train_predict[train]))\n",
        "      print('Fold ',i + 1,' CV AUC : ',roc_auc_score(Y_valid, cv_predict[cv]))\n",
        "            \n",
        "      del X_train, Y_train, X_valid, Y_valid\n",
        "      gc.collect()\n",
        "\n",
        "    # return a trained model\n",
        "    return model\n",
        "\n",
        "    # --------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    # Function : Create a Submission file\n",
        "    def submission_file(self, model, file_name):\n",
        "      '''\n",
        "        This function creates a submission csv file for a particular model.\n",
        "\n",
        "        Inputs:\n",
        "          self\n",
        "          model : The model for with submission file needs to be created\n",
        "          file_name : With what name should the csv file be stored\n",
        "\n",
        "        Returns:\n",
        "          None\n",
        "      '''\n",
        "      # Creating and storing the submission file for test data for all the features\n",
        "      X_test = test_df\n",
        "      X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "      X_test = X_test.where(X_test.values <= np.finfo(np.float64).max)\n",
        "      X_test.fillna(X_test.median(), inplace = True)\n",
        "\n",
        "      y_pred_lgb = model.predict_proba(X_test)[:, 1]\n",
        "      submit_lgb = pd.read_csv(directory + 'sample_submission.csv')\n",
        "      submit_lgb['TARGET'] = y_pred_lgb\n",
        "      submit_lgb.to_csv(file_name, index = False)\n",
        "\n",
        "  #-----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  def store_model(self, model, file_name):\n",
        "    '''\n",
        "    This function stores the model pickle file to memory.\n",
        "\n",
        "    Inputs:\n",
        "      self\n",
        "      model : model to store\n",
        "      file_name : Name of the model pickly file\n",
        "\n",
        "    Returns\n",
        "      None\n",
        "    '''\n",
        "    pd.to_pickle(model, file_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gte1dP-6W8Cz",
        "outputId": "428daea6-15ac-4deb-8eac-4b25d3bdbc1a"
      },
      "source": [
        "test = deployment_model_training(file_directory='../data/')\n",
        "test.load_data(verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please wait: Loading the data into memory...\n",
            "Data successfully loaded!\n",
            "It took 10.49 seconds to load the data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Do2qLZNXUG4",
        "outputId": "3a1790fd-8ef0-49ff-a8d9-fe87fdfaf83e"
      },
      "source": [
        "train_df, test_df = test.preprocessing(verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Handling Missing values and outliers......\n",
            "Done with pre processing! Time Elapsed : 0.49 seconds\n",
            "Creating new and better features......\n",
            "Done with Feature Engineering! Time Elapsed : 3.62 seconds\n",
            "(356251, 25)\n",
            "(338857, 81)\n",
            "The shape of Final training data : (307507, 105)\n",
            "The shape of Final test data : (48744, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwdU4UAZXehi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0c0485-b768-4773-8d94-e031ab8b42e6"
      },
      "source": [
        "lgbm_model = test.lgbm_model(train_df, test_df, nfolds = 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds.\n",
            "[200]\ttraining's binary_logloss: 0.252879\ttraining's auc: 0.741828\tvalid_1's binary_logloss: 0.259711\tvalid_1's auc: 0.707623\n",
            "[400]\ttraining's binary_logloss: 0.24603\ttraining's auc: 0.765186\tvalid_1's binary_logloss: 0.257598\tvalid_1's auc: 0.715205\n",
            "[600]\ttraining's binary_logloss: 0.241396\ttraining's auc: 0.78141\tvalid_1's binary_logloss: 0.257099\tvalid_1's auc: 0.717085\n",
            "[800]\ttraining's binary_logloss: 0.237016\ttraining's auc: 0.796292\tvalid_1's binary_logloss: 0.25679\tvalid_1's auc: 0.718073\n",
            "[1000]\ttraining's binary_logloss: 0.233167\ttraining's auc: 0.808843\tvalid_1's binary_logloss: 0.256811\tvalid_1's auc: 0.718026\n",
            "Early stopping, best iteration is:\n",
            "[895]\ttraining's binary_logloss: 0.235194\ttraining's auc: 0.802311\tvalid_1's binary_logloss: 0.256763\tvalid_1's auc: 0.718137\n",
            "Fold  1  TRAIN AUC :  0.8023108885635406\n",
            "Fold  1  CV AUC :  0.7181371423199652\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "[200]\ttraining's binary_logloss: 0.252794\ttraining's auc: 0.741776\tvalid_1's binary_logloss: 0.259494\tvalid_1's auc: 0.709642\n",
            "[400]\ttraining's binary_logloss: 0.245627\ttraining's auc: 0.76629\tvalid_1's binary_logloss: 0.2574\tvalid_1's auc: 0.717377\n",
            "[600]\ttraining's binary_logloss: 0.241346\ttraining's auc: 0.782041\tvalid_1's binary_logloss: 0.256873\tvalid_1's auc: 0.719506\n",
            "[800]\ttraining's binary_logloss: 0.237242\ttraining's auc: 0.796041\tvalid_1's binary_logloss: 0.256567\tvalid_1's auc: 0.720885\n",
            "[1000]\ttraining's binary_logloss: 0.23351\ttraining's auc: 0.808487\tvalid_1's binary_logloss: 0.256484\tvalid_1's auc: 0.721275\n",
            "[1200]\ttraining's binary_logloss: 0.230258\ttraining's auc: 0.81893\tvalid_1's binary_logloss: 0.256581\tvalid_1's auc: 0.720811\n",
            "Early stopping, best iteration is:\n",
            "[1043]\ttraining's binary_logloss: 0.232728\ttraining's auc: 0.811068\tvalid_1's binary_logloss: 0.256462\tvalid_1's auc: 0.721388\n",
            "Fold  2  TRAIN AUC :  0.8110676242278902\n",
            "Fold  2  CV AUC :  0.7213877485613021\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "[200]\ttraining's binary_logloss: 0.252532\ttraining's auc: 0.743054\tvalid_1's binary_logloss: 0.25999\tvalid_1's auc: 0.706651\n",
            "[400]\ttraining's binary_logloss: 0.245658\ttraining's auc: 0.766161\tvalid_1's binary_logloss: 0.257798\tvalid_1's auc: 0.714808\n",
            "[600]\ttraining's binary_logloss: 0.240977\ttraining's auc: 0.782549\tvalid_1's binary_logloss: 0.257189\tvalid_1's auc: 0.717138\n",
            "[800]\ttraining's binary_logloss: 0.236955\ttraining's auc: 0.796377\tvalid_1's binary_logloss: 0.257081\tvalid_1's auc: 0.717342\n",
            "[1000]\ttraining's binary_logloss: 0.23332\ttraining's auc: 0.808268\tvalid_1's binary_logloss: 0.257054\tvalid_1's auc: 0.717548\n",
            "Early stopping, best iteration is:\n",
            "[911]\ttraining's binary_logloss: 0.234817\ttraining's auc: 0.80339\tvalid_1's binary_logloss: 0.257029\tvalid_1's auc: 0.717652\n",
            "Fold  3  TRAIN AUC :  0.8033896694227131\n",
            "Fold  3  CV AUC :  0.7176515554117449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGaShw6Z0oUY"
      },
      "source": [
        "test.store_model(lgbm_model, 'lgbm_deployment_model.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hizuYJIhVVaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1ec053-de97-4382-fadf-4cbf5647fa7f"
      },
      "source": [
        "y = train_df.pop('TARGET')\n",
        "X = train_df\n",
        "\n",
        "fpr, tpr, threshold = roc_curve(y, lgbm_model.predict_proba(X)[:,1])\n",
        "j_stat = tpr - fpr\n",
        "best_threshold = threshold[np.argmax(j_stat)]\n",
        "print(f\"Best Threshold = {best_threshold}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Threshold = 0.08347541315538703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlG6jWS704N3"
      },
      "source": [
        "class final_submission:\n",
        "  '''\n",
        "  This class deals with the final submission for the case study.\n",
        "  It contains 4 functions:\n",
        "\n",
        "    1. init function for initialization\n",
        "    2. Pre processing function for data pre processing\n",
        "    3. final_function_1 takes a datapoint as input and gives a prediction as output. \n",
        "    4. final_function_2 takes datapoint(s) as input, target values and gives a metric as output (AUC score)\n",
        "  '''\n",
        "  \n",
        "  # Function 1 : Initialization\n",
        "  def __init__(self, model):\n",
        "      '''\n",
        "      Initialization Function\n",
        "      '''\n",
        "      self.model = model\n",
        "\n",
        "  # ----------------------------------------------------------------------------------------------------------\n",
        "  # Function 2 : Creating new features \n",
        "  def feature_engineering(self, app_data, prev_data):\n",
        "      '''\n",
        "      This function preprocesses the loaded data. The processing includes:\n",
        "\n",
        "        4. Feature Engineering to create some better features\n",
        "\n",
        "      Inputs:\n",
        "        self\n",
        "        app_data : application data\n",
        "        prev_data : previous applications data\n",
        "      \n",
        "      Returns:\n",
        "        final_datapoint : final_datapoint for testing\n",
        "      '''\n",
        "    \n",
        "      # --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "      # Task : Feature Engineering\n",
        "      # --------------------------------------------------------------------------------------------------------------\n",
        "      \n",
        "      # i) Application_train/test Feature Engineering\n",
        "      app_data['NEW_CREDIT_TO_ANNUITY_RATIO'] = app_data['AMT_CREDIT'] / app_data['AMT_ANNUITY']\n",
        "      app_data['NEW_CREDIT_TO_GOODS_RATIO'] = app_data['AMT_CREDIT'] / app_data['AMT_GOODS_PRICE']\n",
        "      app_data['INCOME_PER_PERSON'] = app_data['AMT_INCOME_TOTAL'] / app_data['CNT_FAM_MEMBERS']\n",
        "      app_data['ANNUITY_INCOME_PERC'] = app_data['AMT_ANNUITY'] / app_data['AMT_INCOME_TOTAL']\n",
        "      app_data['INCOME_CREDIT_PERC'] = app_data['AMT_INCOME_TOTAL'] / app_data['AMT_CREDIT']\n",
        "      app_data['DAYS_EMPLOYED_PERC'] = app_data['YEARS_EMPLOYED'] / app_data['AGE']\n",
        "      app_data['PAYMENT_RATE'] = app_data['AMT_ANNUITY'] / app_data['AMT_CREDIT']\n",
        "\n",
        "      # Handling Categorical Data\n",
        "      # 1. Gender\n",
        "      if app_data['CODE_GENDER'] == 'F':\n",
        "        app_data['CODE_GENDER_F'] = 1\n",
        "        app_data['CODE_GENDER_M'] = 0\n",
        "        app_data['CODE_GENDER_nan'] = 0\n",
        "        app_data.drop(labels = ['CODE_GENDER'], inplace = True)\n",
        "\n",
        "      elif app_data['CODE_GENDER'] == 'M':\n",
        "        app_data['CODE_GENDER_F'] = 0\n",
        "        app_data['CODE_GENDER_M'] = 1\n",
        "        app_data['CODE_GENDER_nan'] = 0\n",
        "        app_data.drop(labels = ['CODE_GENDER'], inplace = True)\n",
        "      \n",
        "      else:\n",
        "        app_data['CODE_GENDER_F'] = 0\n",
        "        app_data['CODE_GENDER_M'] = 0\n",
        "        app_data['CODE_GENDER_nan'] = 1\n",
        "        app_data.drop(labels = ['CODE_GENDER'], inplace = True)\n",
        "\n",
        "      # 2. FLAG_OWN_CAR\n",
        "      if app_data['FLAG_OWN_CAR'] == 'N':\n",
        "        app_data['FLAG_OWN_CAR_N'] = 1\n",
        "        app_data['FLAG_OWN_CAR_Y'] = 0\n",
        "        app_data['FLAG_OWN_CAR_nan'] = 0\n",
        "        app_data.drop(labels = ['FLAG_OWN_CAR'], inplace = True)\n",
        "      \n",
        "      elif app_data['FLAG_OWN_CAR'] == 'Y':\n",
        "        app_data['FLAG_OWN_CAR_N'] = 0\n",
        "        app_data['FLAG_OWN_CAR_Y'] = 1\n",
        "        app_data['FLAG_OWN_CAR_nan'] = 0\n",
        "        app_data.drop(labels = ['FLAG_OWN_CAR'], inplace = True)\n",
        "\n",
        "      else:\n",
        "        app_data['FLAG_OWN_CAR_N'] = 0\n",
        "        app_data['FLAG_OWN_CAR_Y'] = 1\n",
        "        app_data['FLAG_OWN_CAR_nan'] = 0\n",
        "        app_data.drop(labels = ['FLAG_OWN_CAR'], inplace = True)\n",
        "\n",
        "\n",
        "      # ii) Previous Application Data\n",
        "      prev_data['APP_CREDIT_PERC'] = prev_data['AMT_APPLICATION'] / prev_data['AMT_CREDIT']\n",
        "\n",
        "      # Handling Categorical Data : Only One Cat column : NAME_CONTRACT_STATUS\n",
        "      prev_data['NAME_CONTRACT_STATUS_Approved'] = np.where(prev_data['NAME_CONTRACT_STATUS'] == 'Approved', 1, 0)\n",
        "      prev_data['NAME_CONTRACT_STATUS_Refused'] = np.where(prev_data['NAME_CONTRACT_STATUS'] == 'Refused', 1, 0)\n",
        "      prev_data['NAME_CONTRACT_STATUS_nan'] = np.where(prev_data['NAME_CONTRACT_STATUS'] == np.nan, 1, 0)\n",
        "      prev_data.drop(columns = ['NAME_CONTRACT_STATUS'], inplace= True)\n",
        "\n",
        "      # Aggregations we will perform \n",
        "      prev_app_num_agg = {\n",
        "                      'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
        "                      'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
        "                      'AMT_CREDIT': ['min', 'max', 'mean'],\n",
        "                      'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
        "                      'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "                      'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
        "                      'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "                      'DAYS_DECISION': ['min', 'max', 'mean'],\n",
        "                      'CNT_PAYMENT': ['mean', 'sum']\n",
        "                    }\n",
        "      \n",
        "      prev_data['ID'] = 1\n",
        "\n",
        "      # Perform the aggregations based on the sk_id_curr\n",
        "      prev_agg_features = prev_data.groupby('ID').agg(prev_app_num_agg)\n",
        "      prev_agg_features.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg_features.columns.tolist()])\n",
        "\n",
        "      # Previous Applications: Approved Applications - only numerical features\n",
        "      approved_applications = prev_data[prev_data['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
        "      approved_agg_features = approved_applications.groupby('ID').agg(prev_app_num_agg)\n",
        "      approved_agg_features.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg_features.columns.tolist()])\n",
        "      prev_agg_features = prev_agg_features.join(approved_agg_features, how='left', on='ID')\n",
        "      \n",
        "      # Previous Applications: Refused Applications - only numerical features\n",
        "      refused_applications = prev_data[prev_data['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
        "      refused_agg_features = refused_applications.groupby('ID').agg(prev_app_num_agg)\n",
        "      refused_agg_features.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg_features.columns.tolist()])\n",
        "      prev_agg_features = prev_agg_features.join(refused_agg_features, how='left', on='ID')\n",
        "\n",
        "      return app_data, prev_agg_features.iloc[0]\n",
        "\n",
        "  # -------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Function 3 : Final Function 1\n",
        "\n",
        "  def final_function_1(self, data):\n",
        "      '''\n",
        "      This function takes a datapoint as input and a prediction as output. \n",
        "\n",
        "      Inputs:\n",
        "        self\n",
        "        datapoint : Input datapoint\n",
        "      \n",
        "      Returns:\n",
        "        pred : predicted value\n",
        "      '''\n",
        "      start_time = time()\n",
        "\n",
        "      pred_values = np.zeros(len(data))\n",
        "      \n",
        "      pred_values = self.model.predict_proba(data)[:,1]\n",
        "      predicted_label = np.where(pred_values > 0.0834754, 1, 0)\n",
        "        \n",
        "      #printing the results\n",
        "      print(\"-\" * 100)\n",
        "      print(f\"Predicted Probabilties for given Client(s) being Defaulter is/are: {np.round(pred_values, 4)}\")\n",
        "      print(f\"The class label for given query point is: {predicted_label}\")\n",
        "      print(f\"Total Time Taken for prediction = {time() - start_time}\")\n",
        "      print('-' * 100)\n",
        "\n",
        "      return pred_values\n",
        "  \n",
        "  # ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  # Function 4 : Final Function 2\n",
        "  def final_function_2(self, data, target):\n",
        "        start_time = time()\n",
        "        #making predictions\n",
        "        pred_values = np.zeros(data.shape[0])\n",
        "        pred_values = model.predict_proba(data)[:,1]\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"Predicted Probabilties for given Client(s) being Defaulter is/are:\\n{np.round(pred_values, 4)}\")\n",
        "        predicted_labels = np.where(pred_values > 0.083, 1, 0)\n",
        "        print(f\"\\nThe predicted class labels are:\\n{predicted_labels}\")\n",
        "        #if there are more than 1 input, printing the Metrics\n",
        "        if len(data)>1:\n",
        "            print('-' * 100)\n",
        "            print(\"\\nThe Performance Metrics are:\\n\")\n",
        "            try:\n",
        "                print(f\"ROC-AUC Score = {roc_auc_score(target, pred_values)}\")\n",
        "            except:\n",
        "                print(\"Cannot calculate ROC-AUC Score as the Test Datapoints have only 1 type of Class Labels present. Try with different Datapoints\")\n",
        "            print(f\"Recall Score = {recall_score(target, predicted_labels)}\")\n",
        "        print(f\"Total Time taken for prediction = {time() - start_time}\")\n",
        "        print(\"-\" * 100)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}